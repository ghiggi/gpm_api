# -----------------------------------------------------------------------------.
# MIT License

# Copyright (c) 2024 GPM-API developers
#
# This file is part of GPM-API.

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# -----------------------------------------------------------------------------.
"""This module provide utilities for the creation of GPM Geographic Buckets."""
import math
import time

import dask
import dask.dataframe as dd
import numpy as np
import pandas as pd
import pyarrow as pa
import xarray as xr

import gpm_api
from gpm_api.bucket.io import get_filepaths_by_bin
from gpm_api.dataset.granule import remove_unused_var_dims
from gpm_api.utils.timing import print_task_elapsed_time


def has_unique_chunking(ds):
    """Check if a dataset has unique chunking."""
    if not isinstance(ds, xr.Dataset):
        raise ValueError("Input must be an xarray Dataset.")

    # Create a dictionary to store unique chunk shapes for each dimension
    unique_chunks_per_dim = {}

    # Iterate through each variable's chunks
    for var_name in ds.variables:
        if hasattr(ds[var_name].data, "chunks"):  # is dask array
            var_chunks = ds[var_name].data.chunks
            for dim, chunks in zip(ds[var_name].dims, var_chunks):
                if dim not in unique_chunks_per_dim:
                    unique_chunks_per_dim[dim] = set()
                    unique_chunks_per_dim[dim].add(chunks)
                if chunks not in unique_chunks_per_dim[dim]:
                    return False

    # If all chunks are unique for each dimension, return True
    return True


def ensure_unique_chunking(ds):
    """Ensure the dataset has unique chunking.

    Conversion to dask.dataframe requires unique chunking.
    If the xr.Dataset does not have unique chunking, perform ds.unify_chunks.

    Variable chunks can be visualized with:

    for var in ds.data_vars:
        print(var, ds[var].chunks)

    """
    if not has_unique_chunking(ds):
        ds = ds.unify_chunks()

    return ds


def get_df_object_columns(df):
    """Get the dataframe columns which have 'object' type."""
    return list(df.select_dtypes(include="object").columns)


def ensure_pyarrow_string_columns(df):
    """Convert 'object' type columns to pyarrow strings."""
    for column in get_df_object_columns(df):
        df[column] = df[column].astype("string[pyarrow]")
    return df


def drop_undesired_columns(df):
    """Drop undesired columns like dataset dimensions without coordinates."""
    undesired_columns = ["cross_track", "along_track", "crsWGS84"]
    undesired_columns = [column for column in undesired_columns if column in df.columns]
    df = df.drop(columns=undesired_columns)
    return df


def ds_to_pd_df_function(ds):
    """Default function to convert an xr.Dataset to a pandas.Dataframe."""
    # Drop unrelevant coordinates
    ds = remove_unused_var_dims(ds)

    # Convert to pandas dataframe
    # - strings are converted to object !
    df = ds.to_dataframe(dim_order=None)

    # Convert object columns to pyarrow string
    df = ensure_pyarrow_string_columns(df)

    # Remove MultiIndex
    df = df.reset_index()

    # Drop unrequired columns (previous dataset dimensions)
    df = drop_undesired_columns(df)
    return df


def ds_to_dask_df_function(ds):
    """Default function to convert an xr.Dataset to a dask.Dataframe."""
    # Drop unrelevant coordinates
    ds = remove_unused_var_dims(ds)

    # Check dataset uniform chunking
    ds = ensure_unique_chunking(ds)

    # Convert to to dask dataframe
    # - strings are converted to object !
    with dask.config.set(**{"array.slicing.split_large_chunks": True}):
        df = ds.to_dask_dataframe(dim_order=None, set_index=False)

    # Convert object columns to pyarrow string
    df = ensure_pyarrow_string_columns(df)

    # Drop unrequired columns (previous dataset dimensions)
    df = drop_undesired_columns(df)
    return df


def _check_is_callable_or_none(argument, argument_name):
    if not (callable(argument) or argument is None):
        raise TypeError(f"{argument_name} must be a function (or None).")


def convert_ds_to_df(
    ds, preprocessing_function, ds_to_df_function, filtering_function, precompute_granule=False
):
    # Check inputs
    _check_is_callable_or_none(preprocessing_function, argument_name="preprocessing_function")
    _check_is_callable_or_none(ds_to_df_function, argument_name="ds_to_df_function")
    _check_is_callable_or_none(filtering_function, argument_name="filtering_function")

    if precompute_granule:
        ds = ds.compute()

    # Preprocess xarray Dataset
    if callable(preprocessing_function):
        ds = preprocessing_function(ds)

    # Convert xarray Dataset to dask.Dataframe
    df = ds_to_df_function(ds)

    # Filter the dataset
    if callable(filtering_function):
        df = filtering_function(df)
    return df


def get_granule_dataframe(
    filepath,
    open_granule_kwargs={},
    preprocessing_function=None,
    ds_to_df_function=ds_to_dask_df_function,
    filtering_function=None,
    precompute_granule=False,
):
    # Open granule
    ds = gpm_api.open_granule(filepath, **open_granule_kwargs)

    # Convert to dataframe
    df = convert_ds_to_df(
        ds=ds,
        preprocessing_function=preprocessing_function,
        ds_to_df_function=ds_to_df_function,
        filtering_function=filtering_function,
        precompute_granule=precompute_granule,
    )

    return df


def get_bin_partition(values, bin_size):
    """
    Compute the bins partitioning values.

    Parameters
    ----------
    values : float or array-like
        Values.
    bin_size : float
        Bin size.

    Returns
    -------
    Bin value : float or array-like
        DESCRIPTION.

    """
    return bin_size * np.floor(values / bin_size)


# bin_size = 10
# values = np.array([-180,-176,-175, -174, -171, 170, 166])
# get_bin_partition(values, bin_size)


def assign_spatial_partitions(
    df, xbin_name, ybin_name, xbin_size, ybin_size, x_column="lat", y_column="lon"
):
    """Add partitioning bin columns to dataframe.

    Works for both dask.dataframe and pandas.dataframe.
    """
    # Add spatial partitions columns to dataframe
    partition_columns = {
        xbin_name: get_bin_partition(df[x_column], bin_size=xbin_size),
        ybin_name: get_bin_partition(df[y_column], bin_size=ybin_size),
    }
    df = df.assign(**partition_columns)
    return df


def _convert_size_to_bytes(size_str):
    """Convert human filesizes to bytes.

    Special cases:
     - singular units, e.g., "1 byte"
     - byte vs b
     - yottabytes, zetabytes, etc.
     - with & without spaces between & around units.
     - floats ("5.2 mb")

    To reverse this, see hurry.filesize or the Django filesizeformat template
    filter.

    :param size_str: A human-readable string representing a file size, e.g.,
    "22 megabytes".
    :return: The number of bytes represented by the string.
    """
    multipliers = {
        "kilobyte": 1024,
        "megabyte": 1024**2,
        "gigabyte": 1024**3,
        "terabyte": 1024**4,
        "petabyte": 1024**5,
        "exabyte": 1024**6,
        "zetabyte": 1024**7,
        "yottabyte": 1024**8,
        "kb": 1024,
        "mb": 1024**2,
        "gb": 1024**3,
        "tb": 1024**4,
        "pb": 1024**5,
        "eb": 1024**6,
        "zb": 1024**7,
        "yb": 1024**8,
    }

    for suffix in multipliers:
        size_str = size_str.lower().strip().strip("s")
        if size_str.lower().endswith(suffix):
            return int(float(size_str[0 : -len(suffix)]) * multipliers[suffix])
    else:
        if size_str.endswith("b"):
            size_str = size_str[0:-1]
        elif size_str.endswith("byte"):
            size_str = size_str[0:-4]
    return int(size_str)


# def test_filesize_conversions(self):
#         """Can we convert human filesizes to bytes?"""
#         qa_pairs = [
#             ('58 kb', 59392),
#             ('117 kb', 119808),
#             ('117kb', 119808),
#             ('1 byte', 1),
#             ('1 b', 1),
#             ('117 bytes', 117),
#             ('117  bytes', 117),
#             ('  117 bytes  ', 117),
#             ('117b', 117),
#             ('117bytes', 117),
#             ('1 kilobyte', 1024),
#             ('117 kilobytes', 119808),
#             ('0.7 mb', 734003),
#             ('1mb', 1048576),
#             ('5.2 mb', 5452595),
#         ]
#         for qa in qa_pairs:
#             print("Converting '%s' to bytes..." % qa[0], end='')
#             self.assertEqual(convert_size_to_bytes(qa[0]), qa[1])
#             print('âœ“')


def convert_size_to_bytes(size):
    if not isinstance(size, (str, int)):
        raise TypeError("Expecting a string (i.e. 200MB) or the integer number of bytes.")
    if isinstance(size, int):
        return size
    try:
        size = _convert_size_to_bytes(size)
    except Exception:
        raise ValueError("Impossible to parse {size_str} to the number of bytes.")
    return size


def estimate_row_group_size(df, size="200MB"):
    """Estimate row_group_size parameter based on the desired row group memory size.

    row_group_size is a Parquet argument controlling the number of rows
    in each Apache Parquet File Row Group.
    """
    if isinstance(df, pa.Table):
        memory_used = df.nbytes
    elif isinstance(df, pd.DataFrame):
        memory_used = df.memory_usage().sum()
    else:
        raise NotImplementedError("Unrecognized dataframe type")
    size_bytes = convert_size_to_bytes(size)
    n_rows = len(df)
    memory_per_row = memory_used / n_rows
    row_group_size = math.floor(size_bytes / memory_per_row)
    return row_group_size


# def _get_bin_meta_template(filepath, bin_name):
#     from dask.dataframe.utils import make_meta

#     from gpm_api.bucket.readers import _read_parquet_bin_files

#     template_df = _read_parquet_bin_files([filepath], bin_name=bin_name)
#     meta = make_meta(template_df)
#     return meta


@print_task_elapsed_time(prefix="Bucket Merging Terminated.")
def merge_granule_buckets(
    bucket_base_dir,
    bucket_filepath,
    xbin_name="lonbin",
    ybin_name="latbin",
    row_group_size="500MB",
    compression="snappy",
    compression_level=None,
    **writer_kwargs,
):
    """
     Merge the per-granule bucket archive in a single  optimized archive !

     Parameters
     ----------
     bucket_base_dir : str
         Base directory of the per-granule bucket archive.
     bucket_filepath : str
         File path of the final bucket archive.
     xbin_name : str, optional
         Name of the binned column used to partition the data along the x dimension.
         The default is "lonbin".
     ybin_name : str, optional
         Name of the binned column used to partition the data along the y dimension.
         The default is "latbin".
     row_group_size : TYPE, optional
         Maximum number of rows in each written Parquet row group.
         If specified as a string (i.e. "500 MB"), the equivalent row group size
         number is estimated. The default is "500MB".
    compression : str, optional
         Specify the compression codec, either on a general basis or per-column.
         Valid values: {"none", "snappy", "gzip", "brotli", "lz4", "zstd"}.
         The default is "snappy".
     compression : int or dict, optional
         Specify the compression level for a codec, either on a general basis or per-column.
         If None is passed, arrow selects the compression level for the compression codec in use.
         The compression level has a different meaning for each codec, so you have
         to read the pyArrow documentation of the codec you are using.
         The default is compression_level=None.
     **writer_kwargs: dict
         Other writer options passed to dask.Dataframe.to_parquet, pyarrow.parquet.write_table
         and pyarrow.parquet.ParquetWriter.
         More information available at:
          - https://docs.dask.org/en/stable/generated/dask.dataframe.to_parquet.html
          - https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html
          - https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html

     Returns
     -------
     None.

    """
    from dask.dataframe.utils import make_meta

    from gpm_api.bucket.readers import _read_parquet_bin_files
    from gpm_api.bucket.writers import write_parquet_dataset

    # TODO: remove need of xbin_name and ybin_name  !
    # --> Must be derived from source bucket !

    # Identify Parquet filepaths for each bin
    print("Searching of Parquet files has started.")
    t_i = time.time()
    bin_path_dict = get_filepaths_by_bin(bucket_base_dir)
    n_geographic_bins = len(bin_path_dict)
    t_f = time.time()
    t_elapsed = round((t_f - t_i) / 60, 1)
    print(f"Searching of Parquet files ended. Elapsed time: {t_elapsed} minutes.")
    print(f"{n_geographic_bins} geographic bins to process.")

    # Retrieve list of bins and associated filepaths
    list_bin_names = list(bin_path_dict.keys())
    list_bin_filepaths = list(bin_path_dict.values())

    # Define meta and row_group_size
    template_filepath = list_bin_filepaths[0][0]
    template_bin_name = list_bin_names[0]
    template_df_pd = _read_parquet_bin_files([template_filepath], bin_name=template_bin_name)
    meta = make_meta(template_df_pd)
    row_group_size = estimate_row_group_size(template_df_pd, size=row_group_size)

    # TODO: debug
    # list_bin_names = list_bin_names[0:10]
    # list_bin_filepaths = list_bin_filepaths[0:10]

    # Read dataframes for each geographic bin
    print("Lazy reading of dataframe has started")
    df = dd.from_map(_read_parquet_bin_files, list_bin_filepaths, list_bin_names, meta=meta)

    # Write Parquet Dataset
    print("Parquet Dataset writing has started")
    partitioning = [xbin_name, ybin_name]
    write_parquet_dataset(
        df=df,
        parquet_filepath=bucket_filepath,
        partition_on=partitioning,
        row_group_size=row_group_size,
        compression=compression,
        compression_level=compression_level,
        **writer_kwargs,
    )

    # TODO: Use write_partitioned_dataset instead !
    # --> https://arrow.apache.org/docs/python/generated/pyarrow.dataset.write_dataset.html
    # --> https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_metadata.html
    # write_partitioned_dataset(
    #     df=df,
    #     base_dir,
    #     partitioning,
    #     filename_prefix="part",
    #     format="parquet",
    #     use_threads=True,
    #     **writer_kwargs,
    # )
    print("Parquet Dataset writing has completed")
